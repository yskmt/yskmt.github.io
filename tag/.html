<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>yskmt - </title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">yskmt </a></h1>
                <nav><ul>
                    <li class="active"><a href="/category/.html"></a></li>
                    <li><a href="/category/blog.html">Blog</a></li>
                    <li><a href="/category/etc.html">etc</a></li>
                    <li><a href="/category/learning.html">Learning</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/theano:-multilayer-perception.html">Theano: Multilayer Perception</a></h1>
<footer class="post-info">
        <span>Tue 05 May 2015</span>
<span>| tags: <a href="/tag/.html"></a></span>
</footer><!-- /.post-info --><!-- Status: draft -->

<ul>
<li>
<p>Very usefulr article by NetFlix: http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html</p>
<ul>
<li><a href="https://github.com/JasperSnoek/spearmint">spearmint</a>:
  Hyperparameter tuning by Bayesian optimization</li>
<li><a href="http://www.celeryproject.org/">Celery</a>: distributed task queue library.</li>
</ul>
</li>
<li>
<p>Completed:
  <a href="http://deeplearning.net/tutorial/mlp.html">Multilayer perception tutorial</a>
  with Theano.</p>
<ul>
<li>Feedforward artificial neural network model</li>
<li>"Logistic regression is a special case of the MLP with no hidden
  layer (the input is directly connected to the output) and the
  cross-entropy (sigmoid output) or negative log-likelihood
  (softmax output) loss." from
  <a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/mlp.html">here</a>.</li>
<li>Cross-entropy loss <em>L(f, (x,y))</em> is minimized when <em>x=y</em>.</li>
</ul>
</li>
<li>
<p>To Read: </p>
<ul>
<li><em>Understanding the difficulty of training deep feedforward neural networks</em></li>
<li><em>Practical Bayesian Optimization of Machine Learning Algorithms</em>
  (re-read)</li>
<li><em>Efficient BackProp</em></li>
</ul>
</li>
</ul>                </article>
<p class="paginator">
    Page 1 / 1
</p>
            </aside><!-- /#featured -->
            </ol><!-- /#posts-list -->
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/yskmt">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>